{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "93Ob82QmrLO7"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/abel/onnxt5\")\n",
    "from transformers import T5Tokenizer\n",
    "from datetime import datetime\n",
    "from onnxruntime import InferenceSession, SessionOptions, ExecutionMode\n",
    "from onnxt5 import create_t5_encoder_decoder, GenerativeT5\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pretrained_model = 't5-base' # This can be a pretrained version, or the path to a huggingface model\n",
    "simplified_encoder, decoder_with_lm_head = create_t5_encoder_decoder(pretrained_model)\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_model)\n",
    "generative_t5_pytorch = GenerativeT5(simplified_encoder.cuda(), decoder_with_lm_head.cuda(), tokenizer, cuda=True)\n",
    "\n",
    "\n",
    "decoder_sess = InferenceSession('/home/abel/t5-decoder-with-lm-head.onnx')\n",
    "encoder_sess = InferenceSession('/home/abel/t5-encoder.onnx')\n",
    "options = SessionOptions()\n",
    "options.intra_op_num_threads = 1\n",
    "options.execution_mode = ExecutionMode.ORT_SEQUENTIAL\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_model)\n",
    "generative_t5_onnx = GenerativeT5(encoder_sess, decoder_sess, tokenizer, onnx=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "I-Xx_fpv0ily"
   },
   "outputs": [],
   "source": [
    "pt_means = []\n",
    "pt_stds = []\n",
    "onnx_means = []\n",
    "onnx_stds = []\n",
    "\n",
    "for i in range(2, 1005, 50):\n",
    "  temp_pytorch = []\n",
    "  temp_onnx = []\n",
    "  for o in range(10):\n",
    "    t1 = datetime.now()\n",
    "    generative_t5_pytorch('Start', i, temperature=0.)\n",
    "    t2 = datetime.now()\n",
    "    generative_t5_onnx('Start', i, temperature=0.)\n",
    "    t3 = datetime.now()\n",
    "    pytorch_t = (t2-t1).total_seconds()\n",
    "    onnx_t = (t3-t2).total_seconds()\n",
    "    temp_pytorch.append(pytorch_t)\n",
    "    temp_onnx.append(onnx_t)\n",
    "  pt_means.append(np.mean(temp_pytorch))\n",
    "  pt_stds.append(np.std(temp_pytorch))\n",
    "  onnx_means.append(np.mean(temp_onnx))\n",
    "  onnx_stds.append(np.std(temp_onnx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6_HUyd90q59c"
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.io.renderers.default = 'colab'\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "x = list(range(2, 1005, 50))\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    # Pytorch\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        name='PyTorch',\n",
    "        y=pt_means,\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=pt_stds,\n",
    "            visible=True)\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        name='ONNX',\n",
    "        y=onnx_means,\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=onnx_stds,\n",
    "            visible=True)\n",
    "    ),\n",
    "    ])\n",
    "fig.update_layout(\n",
    "    title=\"Benchmark of Inference time per number of characters to generate (increasing context)\",\n",
    "    xaxis_title=\"Number of characters to generate (with expanding context)\",\n",
    "    yaxis_title=\"Seconds to complete\",\n",
    "    legend_title=\"Framework\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eUKG06TyuBHr"
   },
   "outputs": [],
   "source": [
    "pt_means_emb = []\n",
    "pt_stds_emb = []\n",
    "onnx_means_emb = []\n",
    "onnx_stds_emb = []\n",
    "\n",
    "for i in range(2, 1005, 50):\n",
    "  temp_pytorch = []\n",
    "  temp_onnx = []\n",
    "  for o in range(10):\n",
    "    inputs = torch.tensor([[1] * i]).cuda()\n",
    "    inputs_numpy = inputs.cpu().numpy()\n",
    "    t1 = datetime.now()\n",
    "    _ = decoder_with_lm_head(inputs, simplified_encoder(inputs))\n",
    "    t2 = datetime.now()\n",
    "    encoder_output = encoder_sess.run(None, {\"input_ids\": inputs_numpy})[0]\n",
    "    # To generate the full model's embeddings\n",
    "    decoder_output = decoder_sess.run(None, {\n",
    "                                            \"input_ids\": inputs_numpy,\n",
    "                                            \"encoder_hidden_states\": encoder_output\n",
    "        })[0]\n",
    "    t3 = datetime.now()\n",
    "    pytorch_t = (t2-t1).total_seconds()\n",
    "    onnx_t = (t3-t2).total_seconds()\n",
    "    print(f'{i}: pt {pytorch_t}s, onnx {onnx_t}s')\n",
    "    temp_pytorch.append(pytorch_t)\n",
    "    temp_onnx.append(onnx_t)\n",
    "  pt_means_emb.append(np.mean(temp_pytorch))\n",
    "  pt_stds_emb.append(np.std(temp_pytorch))\n",
    "  onnx_means_emb.append(np.mean(temp_onnx))\n",
    "  onnx_stds_emb.append(np.std(temp_onnx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "io3d2-g1ueeg"
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.io.renderers.default = 'colab'\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "x = list(range(2, 1005, 50))\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    # Pytorch\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        name='PyTorch',\n",
    "        y=pt_means_emb,\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=pt_stds_emb,\n",
    "            visible=True)\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        name='ONNX',\n",
    "        y=onnx_means_emb,\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=onnx_stds_emb,\n",
    "            visible=True)\n",
    "    ),\n",
    "    ])\n",
    "fig.update_layout(\n",
    "    title=\"Benchmark of embedding time per number of characters to embed\",\n",
    "    xaxis_title=\"Number of characters to embed\",\n",
    "    yaxis_title=\"Seconds to complete\",\n",
    "    legend_title=\"Framework\",\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "benchmark_performance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
